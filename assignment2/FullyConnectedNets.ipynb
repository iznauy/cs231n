{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_val: ', (1000, 3, 32, 32))\n",
      "('X_train: ', (49000, 3, 32, 32))\n",
      "('X_test: ', (1000, 3, 32, 32))\n",
      "('y_val: ', (1000,))\n",
      "('y_train: ', (49000,))\n",
      "('y_test: ', (1000,))\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in list(data.items()):\n",
    "  print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76984772881e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  5.39910036865e-11\n",
      "dw error:  9.9042118654e-11\n",
      "db error:  2.41228675681e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 5e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27563491363e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 3e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward:\n",
      "dx error:  6.7505621216e-11\n",
      "dw error:  8.16201557044e-11\n",
      "db error:  7.82672402146e-12\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "print('Testing affine_relu_forward:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.9996027491\n",
      "dx error:  1.40215660067e-09\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.3025458445\n",
      "dx error:  9.38467316199e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be 1e-9\n",
    "print('Testing svm_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "3.4702243556\n",
      "26.5948426952\n",
      "Running numeric gradient check with reg =  0.0\n",
      "3.4702243556\n",
      "3.4702243556\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022405771\n",
      "3.47022465348\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.47022418846\n",
      "3.47022452274\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.4702243192\n",
      "3.47022439199\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022444994\n",
      "3.47022426125\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "3.47022458068\n",
      "3.47022413051\n",
      "W1 relative error: 1.22e-08\n",
      "3.4702243556\n",
      "3.47020470128\n",
      "3.47024400993\n",
      "3.4702121926\n",
      "3.47023651863\n",
      "3.47022590782\n",
      "3.47022280344\n",
      "3.47022745083\n",
      "3.4702212605\n",
      "3.47023054858\n",
      "3.47021816289\n",
      "3.47022029182\n",
      "3.47022841985\n",
      "3.47024939707\n",
      "3.47019931475\n",
      "3.47020471568\n",
      "3.47024399553\n",
      "3.47021211238\n",
      "3.47023659885\n",
      "3.47022591185\n",
      "3.47022279942\n",
      "3.47022745835\n",
      "3.47022125299\n",
      "3.4702305626\n",
      "3.47021814887\n",
      "3.47022028339\n",
      "3.47022842827\n",
      "3.47024944575\n",
      "3.47019926607\n",
      "3.47020473009\n",
      "3.47024398112\n",
      "3.47021203217\n",
      "3.47023667906\n",
      "3.47022591587\n",
      "3.47022279539\n",
      "3.47022746586\n",
      "3.47022124548\n",
      "3.47023057662\n",
      "3.47021813485\n",
      "3.47022027497\n",
      "3.4702284367\n",
      "3.47024949443\n",
      "3.47019921739\n",
      "3.47020474449\n",
      "3.47024396672\n",
      "3.47021195195\n",
      "3.47023675928\n",
      "3.4702259199\n",
      "3.47022279137\n",
      "3.47022747337\n",
      "3.47022123796\n",
      "3.47023059063\n",
      "3.47021812083\n",
      "3.47022026654\n",
      "3.47022844513\n",
      "3.47024954311\n",
      "3.47019916871\n",
      "3.4702047589\n",
      "3.47024395231\n",
      "3.47021187174\n",
      "3.47023683949\n",
      "3.47022592392\n",
      "3.47022278735\n",
      "3.47022748089\n",
      "3.47022123045\n",
      "3.47023060465\n",
      "3.47021810682\n",
      "3.47022025812\n",
      "3.47022845355\n",
      "3.47024959179\n",
      "3.47019912003\n",
      "3.4702047733\n",
      "3.47024393791\n",
      "3.47021179152\n",
      "3.47023691971\n",
      "3.47022592795\n",
      "3.47022278332\n",
      "3.4702274884\n",
      "3.47022122294\n",
      "3.47023061867\n",
      "3.4702180928\n",
      "3.47022024969\n",
      "3.47022846198\n",
      "3.47024964048\n",
      "3.47019907135\n",
      "3.4702047877\n",
      "3.47024392351\n",
      "3.47021171131\n",
      "3.47023699992\n",
      "3.47022593197\n",
      "3.4702227793\n",
      "3.47022749591\n",
      "3.47022121543\n",
      "3.47023063269\n",
      "3.47021807878\n",
      "3.47022024127\n",
      "3.4702284704\n",
      "3.47024968916\n",
      "3.47019902268\n",
      "3.47020480211\n",
      "3.4702439091\n",
      "3.47021163109\n",
      "3.47023708014\n",
      "3.470225936\n",
      "3.47022277527\n",
      "3.47022750343\n",
      "3.47022120791\n",
      "3.47023064671\n",
      "3.47021806477\n",
      "3.47022023285\n",
      "3.47022847883\n",
      "3.47024973784\n",
      "3.470198974\n",
      "3.47020481651\n",
      "3.4702438947\n",
      "3.47021155088\n",
      "3.47023716035\n",
      "3.47022594002\n",
      "3.47022277125\n",
      "3.47022751094\n",
      "3.4702212004\n",
      "3.47023066072\n",
      "3.47021805075\n",
      "3.47022022442\n",
      "3.47022848726\n",
      "3.47024978652\n",
      "3.47019892532\n",
      "3.47020483092\n",
      "3.47024388029\n",
      "3.47021147066\n",
      "3.47023724057\n",
      "3.47022594405\n",
      "3.47022276722\n",
      "3.47022751845\n",
      "3.47022119289\n",
      "3.47023067474\n",
      "3.47021803673\n",
      "3.470220216\n",
      "3.47022849568\n",
      "3.4702498352\n",
      "3.47019887664\n",
      "3.47020484532\n",
      "3.47024386589\n",
      "3.47021139045\n",
      "3.47023732078\n",
      "3.47022594807\n",
      "3.4702227632\n",
      "3.47022752597\n",
      "3.47022118537\n",
      "3.47023068876\n",
      "3.47021802271\n",
      "3.47022020757\n",
      "3.47022850411\n",
      "3.47024988388\n",
      "3.47019882796\n",
      "3.47020485972\n",
      "3.47024385149\n",
      "3.47021131023\n",
      "3.470237401\n",
      "3.4702259521\n",
      "3.47022275917\n",
      "3.47022753348\n",
      "3.47022117786\n",
      "3.47023070278\n",
      "3.4702180087\n",
      "3.47022019915\n",
      "3.47022851254\n",
      "3.47024993257\n",
      "3.47019877928\n",
      "3.47020487413\n",
      "3.47024383708\n",
      "3.47021123002\n",
      "3.47023748121\n",
      "3.47022595612\n",
      "3.47022275515\n",
      "3.47022754099\n",
      "3.47022117035\n",
      "3.4702307168\n",
      "3.47021799468\n",
      "3.47022019072\n",
      "3.47022852096\n",
      "3.47024998125\n",
      "3.4701987306\n",
      "3.47020488853\n",
      "3.47024382268\n",
      "3.4702111498\n",
      "3.47023756143\n",
      "3.47022596015\n",
      "3.47022275112\n",
      "3.47022754851\n",
      "3.47022116284\n",
      "3.47023073082\n",
      "3.47021798066\n",
      "3.4702201823\n",
      "3.47022852939\n",
      "3.47025002993\n",
      "3.47019868192\n",
      "3.47020490294\n",
      "3.47024380827\n",
      "3.47021106959\n",
      "3.47023764164\n",
      "3.47022596417\n",
      "3.4702227471\n",
      "3.47022755602\n",
      "3.47022115532\n",
      "3.47023074483\n",
      "3.47021796664\n",
      "3.47022017387\n",
      "3.47022853781\n",
      "3.47025007861\n",
      "3.47019863324\n",
      "3.47020491734\n",
      "3.47024379387\n",
      "3.47021098937\n",
      "3.47023772186\n",
      "3.4702259682\n",
      "3.47022274307\n",
      "3.47022756353\n",
      "3.47022114781\n",
      "3.47023075885\n",
      "3.47021795263\n",
      "3.47022016545\n",
      "3.47022854624\n",
      "3.47025012729\n",
      "3.47019858456\n",
      "3.47020493175\n",
      "3.47024377947\n",
      "3.47021090916\n",
      "3.47023780207\n",
      "3.47022597222\n",
      "3.47022273905\n",
      "3.47022757105\n",
      "3.4702211403\n",
      "3.47023077287\n",
      "3.47021793861\n",
      "3.47022015703\n",
      "3.47022855467\n",
      "3.47025017597\n",
      "3.47019853588\n",
      "3.47020494615\n",
      "3.47024376506\n",
      "3.47021082894\n",
      "3.47023788229\n",
      "3.47022597625\n",
      "3.47022273503\n",
      "3.47022757856\n",
      "3.47022113278\n",
      "3.47023078689\n",
      "3.47021792459\n",
      "3.4702201486\n",
      "3.47022856309\n",
      "3.47025022466\n",
      "3.4701984872\n",
      "3.47020496055\n",
      "3.47024375066\n",
      "3.47021074873\n",
      "3.4702379625\n",
      "3.47022598027\n",
      "3.470222731\n",
      "3.47022758608\n",
      "3.47022112527\n",
      "3.47023080091\n",
      "3.47021791058\n",
      "3.47022014018\n",
      "3.47022857152\n",
      "3.47025027334\n",
      "3.47019843852\n",
      "3.47020497496\n",
      "3.47024373625\n",
      "3.47021066851\n",
      "3.47023804272\n",
      "3.4702259843\n",
      "3.47022272698\n",
      "3.47022759359\n",
      "3.47022111776\n",
      "3.47023081492\n",
      "3.47021789656\n",
      "3.47022013175\n",
      "3.47022857994\n",
      "3.47025032202\n",
      "3.47019838984\n",
      "3.47020498936\n",
      "3.47024372185\n",
      "3.4702105883\n",
      "3.47023812293\n",
      "3.47022598832\n",
      "3.47022272295\n",
      "3.4702276011\n",
      "3.47022111025\n",
      "3.47023082894\n",
      "3.47021788254\n",
      "3.47022012333\n",
      "3.47022858837\n",
      "3.4702503707\n",
      "3.47019834116\n",
      "3.47020500377\n",
      "3.47024370745\n",
      "3.47021050808\n",
      "3.47023820315\n",
      "3.47022599235\n",
      "3.47022271893\n",
      "3.47022760862\n",
      "3.47022110273\n",
      "3.47023084296\n",
      "3.47021786852\n",
      "3.4702201149\n",
      "3.4702285968\n",
      "3.47025041938\n",
      "3.47019829248\n",
      "3.47020501817\n",
      "3.47024369304\n",
      "3.47021042787\n",
      "3.47023828336\n",
      "3.47022599637\n",
      "3.4702227149\n",
      "3.47022761613\n",
      "3.47022109522\n",
      "3.47023085698\n",
      "3.47021785451\n",
      "3.47022010648\n",
      "3.47022860522\n",
      "3.47025046806\n",
      "3.4701982438\n",
      "3.47020503257\n",
      "3.47024367864\n",
      "3.47021034765\n",
      "3.47023836358\n",
      "3.4702260004\n",
      "3.47022271088\n",
      "3.47022762364\n",
      "3.47022108771\n",
      "3.470230871\n",
      "3.47021784049\n",
      "3.47022009805\n",
      "3.47022861365\n",
      "3.47025051675\n",
      "3.47019819512\n",
      "3.47020504698\n",
      "3.47024366423\n",
      "3.47021026744\n",
      "3.47023844379\n",
      "3.47022600442\n",
      "3.47022270685\n",
      "3.47022763116\n",
      "3.47022108019\n",
      "3.47023088502\n",
      "3.47021782647\n",
      "3.47022008963\n",
      "3.47022862208\n",
      "3.47025056543\n",
      "3.47019814645\n",
      "3.47020506138\n",
      "3.47024364983\n",
      "3.47021018722\n",
      "3.47023852401\n",
      "3.47022600845\n",
      "3.47022270283\n",
      "3.47022763867\n",
      "3.47022107268\n",
      "3.47023089903\n",
      "3.47021781246\n",
      "3.47022008121\n",
      "3.4702286305\n",
      "3.47025061411\n",
      "3.47019809777\n",
      "3.47020507579\n",
      "3.47024363543\n",
      "3.47021010701\n",
      "3.47023860422\n",
      "3.47022601247\n",
      "3.4702226988\n",
      "3.47022764618\n",
      "3.47022106517\n",
      "3.47023091305\n",
      "3.47021779844\n",
      "3.47022007278\n",
      "3.47022863893\n",
      "3.47025066279\n",
      "3.47019804909\n",
      "3.47020509019\n",
      "3.47024362102\n",
      "3.47021002679\n",
      "3.47023868444\n",
      "3.4702260165\n",
      "3.47022269478\n",
      "3.4702276537\n",
      "3.47022105765\n",
      "3.47023092707\n",
      "3.47021778442\n",
      "3.47022006436\n",
      "3.47022864735\n",
      "3.47025071147\n",
      "3.47019800041\n",
      "3.47020510459\n",
      "3.47024360662\n",
      "3.47020994658\n",
      "3.47023876465\n",
      "3.47022602052\n",
      "3.47022269075\n",
      "3.47022766121\n",
      "3.47022105014\n",
      "3.47023094109\n",
      "3.4702177704\n",
      "3.47022005593\n",
      "3.47022865578\n",
      "3.47025076015\n",
      "3.47019795173\n",
      "3.470205119\n",
      "3.47024359221\n",
      "3.47020986636\n",
      "3.47023884487\n",
      "3.47022602455\n",
      "3.47022268673\n",
      "3.47022766872\n",
      "3.47022104263\n",
      "3.47023095511\n",
      "3.47021775639\n",
      "3.47022004751\n",
      "3.47022866421\n",
      "3.47025080884\n",
      "3.47019790305\n",
      "3.4702051334\n",
      "3.47024357781\n",
      "3.47020978615\n",
      "3.47023892508\n",
      "3.47022602857\n",
      "3.47022268271\n",
      "3.47022767624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.47022103512\n",
      "3.47023096913\n",
      "3.47021774237\n",
      "3.47022003908\n",
      "3.47022867263\n",
      "3.47025085752\n",
      "3.47019785437\n",
      "3.47020514781\n",
      "3.47024356341\n",
      "3.47020970593\n",
      "3.4702390053\n",
      "3.4702260326\n",
      "3.47022267868\n",
      "3.47022768375\n",
      "3.4702210276\n",
      "3.47023098314\n",
      "3.47021772835\n",
      "3.47022003066\n",
      "3.47022868106\n",
      "3.4702509062\n",
      "3.47019780569\n",
      "3.47020516221\n",
      "3.470243549\n",
      "3.47020962572\n",
      "3.47023908552\n",
      "3.47022603662\n",
      "3.47022267466\n",
      "3.47022769126\n",
      "3.47022102009\n",
      "3.47023099716\n",
      "3.47021771434\n",
      "3.47022002224\n",
      "3.47022868948\n",
      "3.47025095488\n",
      "3.47019775701\n",
      "3.47020517662\n",
      "3.4702435346\n",
      "3.4702095455\n",
      "3.47023916573\n",
      "3.47022604065\n",
      "3.47022267063\n",
      "3.47022769878\n",
      "3.47022101258\n",
      "3.47023101118\n",
      "3.47021770032\n",
      "3.47022001381\n",
      "3.47022869791\n",
      "3.47025100356\n",
      "3.47019770833\n",
      "3.47020519102\n",
      "3.47024352019\n",
      "3.47020946529\n",
      "3.47023924595\n",
      "3.47022604467\n",
      "3.47022266661\n",
      "3.47022770629\n",
      "3.47022100506\n",
      "3.4702310252\n",
      "3.4702176863\n",
      "3.47022000539\n",
      "3.47022870634\n",
      "3.47025105224\n",
      "3.47019765965\n",
      "3.47020520542\n",
      "3.47024350579\n",
      "3.47020938507\n",
      "3.47023932616\n",
      "3.4702260487\n",
      "3.47022266258\n",
      "3.47022771381\n",
      "3.47022099755\n",
      "3.47023103922\n",
      "3.47021767228\n",
      "3.47021999696\n",
      "3.47022871476\n",
      "3.47025110093\n",
      "3.47019761097\n",
      "3.47020521983\n",
      "3.47024349139\n",
      "3.47020930486\n",
      "3.47023940638\n",
      "3.47022605272\n",
      "3.47022265856\n",
      "3.47022772132\n",
      "3.47022099004\n",
      "3.47023105324\n",
      "3.47021765827\n",
      "3.47021998854\n",
      "3.47022872319\n",
      "3.47025114961\n",
      "3.47019756229\n",
      "3.47020523423\n",
      "3.47024347698\n",
      "3.47020922464\n",
      "3.47023948659\n",
      "3.47022605674\n",
      "3.47022265453\n",
      "3.47022772883\n",
      "3.47022098253\n",
      "3.47023106725\n",
      "3.47021764425\n",
      "3.47021998011\n",
      "3.47022873162\n",
      "3.47025119829\n",
      "3.47019751361\n",
      "3.47020524864\n",
      "3.47024346258\n",
      "3.47020914443\n",
      "3.47023956681\n",
      "3.47022606077\n",
      "3.47022265051\n",
      "3.47022773635\n",
      "3.47022097501\n",
      "3.47023108127\n",
      "3.47021763023\n",
      "3.47021997169\n",
      "3.47022874004\n",
      "3.47025124697\n",
      "3.47019746493\n",
      "3.47020526304\n",
      "3.47024344817\n",
      "3.47020906421\n",
      "3.47023964702\n",
      "3.47022606479\n",
      "3.47022264648\n",
      "3.47022774386\n",
      "3.4702209675\n",
      "3.47023109529\n",
      "3.47021761622\n",
      "3.47021996326\n",
      "3.47022874847\n",
      "3.47025129565\n",
      "3.47019741626\n",
      "3.47020527744\n",
      "3.47024343377\n",
      "3.470208984\n",
      "3.47023972724\n",
      "3.47022606882\n",
      "3.47022264246\n",
      "3.47022775137\n",
      "3.47022095999\n",
      "3.47023110931\n",
      "3.4702176022\n",
      "3.47021995484\n",
      "3.47022875689\n",
      "3.47025134434\n",
      "3.47019736758\n",
      "3.47020529185\n",
      "3.47024341937\n",
      "3.47020890378\n",
      "3.47023980745\n",
      "3.47022607284\n",
      "3.47022263843\n",
      "3.47022775889\n",
      "3.47022095247\n",
      "3.47023112333\n",
      "3.47021758818\n",
      "3.47021994642\n",
      "3.47022876532\n",
      "3.47025139302\n",
      "3.4701973189\n",
      "3.47020530625\n",
      "3.47024340496\n",
      "3.47020882357\n",
      "3.47023988767\n",
      "3.47022607687\n",
      "3.47022263441\n",
      "3.4702277664\n",
      "3.47022094496\n",
      "3.47023113734\n",
      "3.47021757417\n",
      "3.47021993799\n",
      "3.47022877375\n",
      "3.4702514417\n",
      "3.47019727022\n",
      "3.47020532066\n",
      "3.47024339056\n",
      "3.47020874335\n",
      "3.47023996788\n",
      "3.47022608089\n",
      "3.47022263039\n",
      "3.47022777391\n",
      "3.47022093745\n",
      "3.47023115136\n",
      "3.47021756015\n",
      "3.47021992957\n",
      "3.47022878217\n",
      "3.47025149038\n",
      "3.47019722154\n",
      "3.47020533506\n",
      "3.47024337615\n",
      "3.47020866314\n",
      "3.4702400481\n",
      "3.47022608492\n",
      "3.47022262636\n",
      "3.47022778143\n",
      "3.47022092994\n",
      "3.47023116538\n",
      "3.47021754613\n",
      "3.47021992114\n",
      "3.4702287906\n",
      "3.47025153906\n",
      "3.47019717286\n",
      "3.47020534947\n",
      "3.47024336175\n",
      "3.47020858292\n",
      "3.47024012831\n",
      "3.47022608894\n",
      "3.47022262234\n",
      "3.47022778894\n",
      "3.47022092242\n",
      "3.4702311794\n",
      "3.47021753211\n",
      "3.47021991272\n",
      "3.47022879903\n",
      "3.47025158775\n",
      "3.47019712418\n",
      "3.47020536387\n",
      "3.47024334734\n",
      "3.47020850271\n",
      "3.47024020853\n",
      "3.47022609297\n",
      "3.47022261831\n",
      "3.47022779645\n",
      "3.47022091491\n",
      "3.47023119342\n",
      "3.4702175181\n",
      "3.47021990429\n",
      "3.47022880745\n",
      "3.47025163643\n",
      "3.4701970755\n",
      "3.47020537827\n",
      "3.47024333294\n",
      "3.47020842249\n",
      "3.47024028874\n",
      "3.47022609699\n",
      "3.47022261429\n",
      "3.47022780397\n",
      "3.4702209074\n",
      "3.47023120744\n",
      "3.47021750408\n",
      "3.47021989587\n",
      "3.47022881588\n",
      "3.47025168511\n",
      "3.47019702682\n",
      "3.47020539268\n",
      "3.47024331854\n",
      "3.47020834228\n",
      "3.47024036896\n",
      "3.47022610102\n",
      "3.47022261026\n",
      "3.47022781148\n",
      "3.47022089988\n",
      "3.47023122145\n",
      "3.47021749006\n",
      "3.47021988745\n",
      "3.4702288243\n",
      "3.47025173379\n",
      "3.47019697814\n",
      "3.47020540708\n",
      "3.47024330413\n",
      "3.47020826206\n",
      "3.47024044917\n",
      "3.47022610504\n",
      "3.47022260624\n",
      "3.470227819\n",
      "3.47022089237\n",
      "3.47023123547\n",
      "3.47021747605\n",
      "3.47021987902\n",
      "3.47022883273\n",
      "3.47025178247\n",
      "3.47019692946\n",
      "W2 relative error: 3.48e-10\n",
      "3.4702243556\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "3.47022441661\n",
      "3.47022429458\n",
      "b1 relative error: 6.55e-09\n",
      "3.4702243556\n",
      "3.4702211062\n",
      "3.47022760499\n",
      "3.47022118683\n",
      "3.47022752437\n",
      "3.47022467928\n",
      "3.47022403191\n",
      "3.47022499439\n",
      "3.47022371681\n",
      "3.47022562051\n",
      "3.47022309069\n",
      "3.47022353557\n",
      "3.47022517564\n",
      "3.47022936642\n",
      "3.47021934479\n",
      "b2 relative error: 4.33e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "19.6574571933\n",
      "19.6574571933\n",
      "19.6574519955\n",
      "19.6574623913\n",
      "19.6574520236\n",
      "19.6574623632\n",
      "19.6574520517\n",
      "19.657462335\n",
      "19.6574520798\n",
      "19.6574623069\n",
      "19.6574521079\n",
      "19.6574622788\n",
      "19.6574521361\n",
      "19.6574622507\n",
      "19.6574521642\n",
      "19.6574622226\n",
      "19.6574521923\n",
      "19.6574621945\n",
      "19.6574522204\n",
      "19.6574621664\n",
      "19.6574522485\n",
      "19.6574621383\n",
      "19.6574522766\n",
      "19.6574621101\n",
      "19.6574523047\n",
      "19.657462082\n",
      "19.6574523328\n",
      "19.6574620539\n",
      "19.657452361\n",
      "19.6574620258\n",
      "19.6574523891\n",
      "19.6574619977\n",
      "19.6574524172\n",
      "19.6574619696\n",
      "19.6574524453\n",
      "19.6574619415\n",
      "19.6574524734\n",
      "19.6574619134\n",
      "19.6574525015\n",
      "19.6574618852\n",
      "19.6574525296\n",
      "19.6574618571\n",
      "19.6574525577\n",
      "19.657461829\n",
      "19.6574525859\n",
      "19.6574618009\n",
      "19.657452614\n",
      "19.6574617728\n",
      "19.6574526421\n",
      "19.6574617447\n",
      "19.6574526702\n",
      "19.6574617166\n",
      "19.6574526983\n",
      "19.6574616885\n",
      "19.6574527264\n",
      "19.6574616603\n",
      "19.6574527545\n",
      "19.6574616322\n",
      "19.6574527826\n",
      "19.6574616041\n",
      "19.6574528108\n",
      "19.657461576\n",
      "19.6574528389\n",
      "19.6574615479\n",
      "19.657452867\n",
      "19.6574615198\n",
      "19.6574528951\n",
      "19.6574614917\n",
      "19.6574529232\n",
      "19.6574614636\n",
      "19.6574529513\n",
      "19.6574614354\n",
      "19.6574529794\n",
      "19.6574614073\n",
      "19.6574530075\n",
      "19.6574613792\n",
      "19.6574530357\n",
      "19.6574613511\n",
      "19.6574530638\n",
      "19.657461323\n",
      "19.6574530919\n",
      "19.6574612949\n",
      "19.65745312\n",
      "19.6574612668\n",
      "19.6574531481\n",
      "19.6574612387\n",
      "19.6574531762\n",
      "19.6574612105\n",
      "19.6574532043\n",
      "19.6574611824\n",
      "19.6574532324\n",
      "19.6574611543\n",
      "19.6574532606\n",
      "19.6574611262\n",
      "19.6574532887\n",
      "19.6574610981\n",
      "19.6574533168\n",
      "19.65746107\n",
      "19.6574533449\n",
      "19.6574610419\n",
      "19.657453373\n",
      "19.6574610138\n",
      "19.6574535319\n",
      "19.6574608549\n",
      "19.65745356\n",
      "19.6574608268\n",
      "19.6574535881\n",
      "19.6574607987\n",
      "19.6574536162\n",
      "19.6574607706\n",
      "19.6574536443\n",
      "19.6574607424\n",
      "19.6574536724\n",
      "19.6574607143\n",
      "19.6574537005\n",
      "19.6574606862\n",
      "19.6574537286\n",
      "19.6574606581\n",
      "19.6574537568\n",
      "19.65746063\n",
      "19.6574537849\n",
      "19.6574606019\n",
      "19.657453813\n",
      "19.6574605738\n",
      "19.6574538411\n",
      "19.6574605457\n",
      "19.6574538692\n",
      "19.6574605175\n",
      "19.6574538973\n",
      "19.6574604894\n",
      "19.6574539254\n",
      "19.6574604613\n",
      "19.6574539535\n",
      "19.6574604332\n",
      "19.6574539817\n",
      "19.6574604051\n",
      "19.6574540098\n",
      "19.657460377\n",
      "19.6574540379\n",
      "19.6574603489\n",
      "19.657454066\n",
      "19.6574603208\n",
      "19.6574540941\n",
      "19.6574602926\n",
      "19.6574541222\n",
      "19.6574602645\n",
      "19.6574541503\n",
      "19.6574602364\n",
      "19.6574541784\n",
      "19.6574602083\n",
      "19.6574542066\n",
      "19.6574601802\n",
      "19.6574542347\n",
      "19.6574601521\n",
      "19.6574542628\n",
      "19.657460124\n",
      "19.6574542909\n",
      "19.6574600959\n",
      "19.657454319\n",
      "19.6574600677\n",
      "19.6574543471\n",
      "19.6574600396\n",
      "19.6574543752\n",
      "19.6574600115\n",
      "19.6574544033\n",
      "19.6574599834\n",
      "19.6574544315\n",
      "19.6574599553\n",
      "19.6574544596\n",
      "19.6574599272\n",
      "19.6574544877\n",
      "19.6574598991\n",
      "19.6574545158\n",
      "19.657459871\n",
      "19.6574545439\n",
      "19.6574598429\n",
      "19.657454572\n",
      "19.6574598147\n",
      "19.6574546001\n",
      "19.6574597866\n",
      "19.6574546282\n",
      "19.6574597585\n",
      "19.6574546564\n",
      "19.6574597304\n",
      "19.6574546845\n",
      "19.6574597023\n",
      "19.6574547126\n",
      "19.6574596742\n",
      "19.6574547407\n",
      "19.6574596461\n",
      "19.6574547688\n",
      "19.657459618\n",
      "19.6574547969\n",
      "19.6574595898\n",
      "19.657454825\n",
      "19.6574595617\n",
      "19.6574548531\n",
      "19.6574595336\n",
      "19.6574548813\n",
      "19.6574595055\n",
      "19.6574549094\n",
      "19.6574594774\n",
      "19.6574550682\n",
      "19.6574593185\n",
      "19.6574550963\n",
      "19.6574592904\n",
      "19.6574551245\n",
      "19.6574592623\n",
      "19.6574551526\n",
      "19.6574592342\n",
      "19.6574551807\n",
      "19.6574592061\n",
      "19.6574552088\n",
      "19.657459178\n",
      "19.6574552369\n",
      "19.6574591499\n",
      "19.657455265\n",
      "19.6574591217\n",
      "19.6574552931\n",
      "19.6574590936\n",
      "19.6574553212\n",
      "19.6574590655\n",
      "19.6574553494\n",
      "19.6574590374\n",
      "19.6574553775\n",
      "19.6574590093\n",
      "19.6574554056\n",
      "19.6574589812\n",
      "19.6574554337\n",
      "19.6574589531\n",
      "19.6574554618\n",
      "19.657458925\n",
      "19.6574554899\n",
      "19.6574588968\n",
      "19.657455518\n",
      "19.6574588687\n",
      "19.6574555461\n",
      "19.6574588406\n",
      "19.6574555743\n",
      "19.6574588125\n",
      "19.6574556024\n",
      "19.6574587844\n",
      "19.6574556305\n",
      "19.6574587563\n",
      "19.6574556586\n",
      "19.6574587282\n",
      "19.6574556867\n",
      "19.6574587001\n",
      "19.6574557148\n",
      "19.6574586719\n",
      "19.6574557429\n",
      "19.6574586438\n",
      "19.657455771\n",
      "19.6574586157\n",
      "19.6574557992\n",
      "19.6574585876\n",
      "19.6574558273\n",
      "19.6574585595\n",
      "19.6574558554\n",
      "19.6574585314\n",
      "19.6574558835\n",
      "19.6574585033\n",
      "19.6574559116\n",
      "19.6574584752\n",
      "19.6574559397\n",
      "19.657458447\n",
      "19.6574559678\n",
      "19.6574584189\n",
      "19.6574559959\n",
      "19.6574583908\n",
      "19.6574560241\n",
      "19.6574583627\n",
      "19.6574560522\n",
      "19.6574583346\n",
      "19.6574560803\n",
      "19.6574583065\n",
      "19.6574561084\n",
      "19.6574582784\n",
      "19.6574561365\n",
      "19.6574582503\n",
      "19.6574561646\n",
      "19.6574582221\n",
      "19.6574561927\n",
      "19.657458194\n",
      "19.6574562208\n",
      "19.6574581659\n",
      "19.657456249\n",
      "19.6574581378\n",
      "19.6574562771\n",
      "19.6574581097\n",
      "19.6574563052\n",
      "19.6574580816\n",
      "19.6574563333\n",
      "19.6574580535\n",
      "19.6574563614\n",
      "19.6574580254\n",
      "19.6574563895\n",
      "19.6574579972\n",
      "19.6574564176\n",
      "19.6574579691\n",
      "19.6574564457\n",
      "19.657457941\n",
      "19.6574566046\n",
      "19.6574577822\n",
      "19.6574566327\n",
      "19.6574577541\n",
      "19.6574566608\n",
      "19.6574577259\n",
      "19.6574566889\n",
      "19.6574576978\n",
      "19.657456717\n",
      "19.6574576697\n",
      "19.6574567452\n",
      "19.6574576416\n",
      "19.6574567733\n",
      "19.6574576135\n",
      "19.6574568014\n",
      "19.6574575854\n",
      "19.6574568295\n",
      "19.6574575573\n",
      "19.6574568576\n",
      "19.6574575292\n",
      "19.6574568857\n",
      "19.657457501\n",
      "19.6574569138\n",
      "19.6574574729\n",
      "19.6574569419\n",
      "19.6574574448\n",
      "19.6574569701\n",
      "19.6574574167\n",
      "19.6574569982\n",
      "19.6574573886\n",
      "19.6574570263\n",
      "19.6574573605\n",
      "19.6574570544\n",
      "19.6574573324\n",
      "19.6574570825\n",
      "19.6574573043\n",
      "19.6574571106\n",
      "19.6574572761\n",
      "19.6574571387\n",
      "19.657457248\n",
      "19.6574571668\n",
      "19.6574572199\n",
      "19.657457195\n",
      "19.6574571918\n",
      "19.6574572231\n",
      "19.6574571637\n",
      "19.6574572512\n",
      "19.6574571356\n",
      "19.6574572793\n",
      "19.6574571075\n",
      "19.6574573074\n",
      "19.6574570794\n",
      "19.6574573355\n",
      "19.6574570512\n",
      "19.6574573636\n",
      "19.6574570231\n",
      "19.6574573917\n",
      "19.657456995\n",
      "19.6574574199\n",
      "19.6574569669\n",
      "19.657457448\n",
      "19.6574569388\n",
      "19.6574574761\n",
      "19.6574569107\n",
      "19.6574575042\n",
      "19.6574568826\n",
      "19.6574575323\n",
      "19.6574568545\n",
      "19.6574575604\n",
      "19.6574568263\n",
      "19.6574575885\n",
      "19.6574567982\n",
      "19.6574576166\n",
      "19.6574567701\n",
      "19.6574576448\n",
      "19.657456742\n",
      "19.6574576729\n",
      "19.6574567139\n",
      "19.657457701\n",
      "19.6574566858\n",
      "19.6574577291\n",
      "19.6574566577\n",
      "19.6574577572\n",
      "19.6574566296\n",
      "19.6574577853\n",
      "19.6574566014\n",
      "19.6574578134\n",
      "19.6574565733\n",
      "19.6574578415\n",
      "19.6574565452\n",
      "19.6574578697\n",
      "19.6574565171\n",
      "19.6574578978\n",
      "19.657456489\n",
      "19.6574579259\n",
      "19.6574564609\n",
      "19.657457954\n",
      "19.6574564328\n",
      "19.6574579821\n",
      "19.6574564047\n",
      "19.657458141\n",
      "19.6574562458\n",
      "19.6574581691\n",
      "19.6574562177\n",
      "19.6574581972\n",
      "19.6574561896\n",
      "19.6574582253\n",
      "19.6574561615\n",
      "19.6574582534\n",
      "19.6574561334\n",
      "19.6574582815\n",
      "19.6574561052\n",
      "19.6574583096\n",
      "19.6574560771\n",
      "19.6574583377\n",
      "19.657456049\n",
      "19.6574583659\n",
      "19.6574560209\n",
      "19.657458394\n",
      "19.6574559928\n",
      "19.6574584221\n",
      "19.6574559647\n",
      "19.6574584502\n",
      "19.6574559366\n",
      "19.6574584783\n",
      "19.6574559085\n",
      "19.6574585064\n",
      "19.6574558803\n",
      "19.6574585345\n",
      "19.6574558522\n",
      "19.6574585626\n",
      "19.6574558241\n",
      "19.6574585908\n",
      "19.657455796\n",
      "19.6574586189\n",
      "19.6574557679\n",
      "19.657458647\n",
      "19.6574557398\n",
      "19.6574586751\n",
      "19.6574557117\n",
      "19.6574587032\n",
      "19.6574556836\n",
      "19.6574587313\n",
      "19.6574556554\n",
      "19.6574587594\n",
      "19.6574556273\n",
      "19.6574587875\n",
      "19.6574555992\n",
      "19.6574588157\n",
      "19.6574555711\n",
      "19.6574588438\n",
      "19.657455543\n",
      "19.6574588719\n",
      "19.6574555149\n",
      "19.6574589\n",
      "19.6574554868\n",
      "19.6574589281\n",
      "19.6574554587\n",
      "19.6574589562\n",
      "19.6574554305\n",
      "19.6574589843\n",
      "19.6574554024\n",
      "19.6574590124\n",
      "19.6574553743\n",
      "19.6574590406\n",
      "19.6574553462\n",
      "19.6574590687\n",
      "19.6574553181\n",
      "19.6574590968\n",
      "19.65745529\n",
      "19.6574591249\n",
      "19.6574552619\n",
      "19.657459153\n",
      "19.6574552338\n",
      "19.6574591811\n",
      "19.6574552056\n",
      "19.6574592092\n",
      "19.6574551775\n",
      "19.6574592373\n",
      "19.6574551494\n",
      "19.6574592655\n",
      "19.6574551213\n",
      "19.6574592936\n",
      "19.6574550932\n",
      "19.6574593217\n",
      "19.6574550651\n",
      "19.6574593498\n",
      "19.657455037\n",
      "19.6574593779\n",
      "19.6574550089\n",
      "19.657459406\n",
      "19.6574549807\n",
      "19.6574594341\n",
      "19.6574549526\n",
      "19.6574594622\n",
      "19.6574549245\n",
      "19.6574594904\n",
      "19.6574548964\n",
      "19.6574595185\n",
      "19.6574548683\n",
      "W1 relative error: 8.18e-07\n",
      "19.6574571933\n",
      "19.6574354391\n",
      "19.6574789477\n",
      "19.6574429444\n",
      "19.6574714424\n",
      "19.6574566737\n",
      "19.6574577131\n",
      "19.6574582307\n",
      "19.6574561562\n",
      "19.6574613425\n",
      "19.6574530445\n",
      "19.6574510998\n",
      "19.6574632874\n",
      "19.6574802191\n",
      "19.6574341683\n",
      "19.6574355517\n",
      "19.657478835\n",
      "19.6574429625\n",
      "19.6574714243\n",
      "19.657456776\n",
      "19.6574576108\n",
      "19.6574583365\n",
      "19.6574560504\n",
      "19.6574614548\n",
      "19.6574529322\n",
      "19.6574511897\n",
      "19.6574631976\n",
      "19.6574803661\n",
      "19.6574340213\n",
      "19.6574356644\n",
      "19.6574787223\n",
      "19.6574429806\n",
      "19.6574714062\n",
      "19.6574568783\n",
      "19.6574575085\n",
      "19.6574584423\n",
      "19.6574559446\n",
      "19.6574615671\n",
      "19.6574528199\n",
      "19.6574512795\n",
      "19.6574631077\n",
      "19.657480513\n",
      "19.6574338744\n",
      "19.6574357771\n",
      "19.6574786097\n",
      "19.6574429986\n",
      "19.6574713882\n",
      "19.6574569806\n",
      "19.6574574062\n",
      "19.6574585481\n",
      "19.6574558388\n",
      "19.6574616794\n",
      "19.6574527076\n",
      "19.6574513694\n",
      "19.6574630179\n",
      "19.65748066\n",
      "19.6574337274\n",
      "19.6574358898\n",
      "19.657478497\n",
      "19.6574430167\n",
      "19.6574713701\n",
      "19.6574570829\n",
      "19.6574573039\n",
      "19.6574586539\n",
      "19.657455733\n",
      "19.6574617917\n",
      "19.6574525953\n",
      "19.6574514592\n",
      "19.657462928\n",
      "19.6574808069\n",
      "19.6574335805\n",
      "19.6574360025\n",
      "19.6574783843\n",
      "19.6574430348\n",
      "19.657471352\n",
      "19.6574571852\n",
      "19.6574572016\n",
      "19.6574587597\n",
      "19.6574556272\n",
      "19.657461904\n",
      "19.657452483\n",
      "19.6574515491\n",
      "19.6574628382\n",
      "19.6574809539\n",
      "19.6574334335\n",
      "19.6574361152\n",
      "19.6574782716\n",
      "19.6574430528\n",
      "19.657471334\n",
      "19.6574572875\n",
      "19.6574570993\n",
      "19.6574588655\n",
      "19.6574555214\n",
      "19.6574620163\n",
      "19.6574523707\n",
      "19.6574516389\n",
      "19.6574627483\n",
      "19.6574811009\n",
      "19.6574332865\n",
      "19.6574362279\n",
      "19.6574781589\n",
      "19.6574430709\n",
      "19.6574713159\n",
      "19.6574573898\n",
      "19.657456997\n",
      "19.6574589713\n",
      "19.6574554156\n",
      "19.6574621286\n",
      "19.6574522584\n",
      "19.6574517288\n",
      "19.6574626584\n",
      "19.6574812478\n",
      "19.6574331396\n",
      "19.6574363405\n",
      "19.6574780462\n",
      "19.657443089\n",
      "19.6574712979\n",
      "19.6574574921\n",
      "19.6574568947\n",
      "19.6574590771\n",
      "19.6574553098\n",
      "19.6574622409\n",
      "19.6574521461\n",
      "19.6574518187\n",
      "19.6574625686\n",
      "19.6574813948\n",
      "19.6574329926\n",
      "19.6574364532\n",
      "19.6574779336\n",
      "19.657443107\n",
      "19.6574712798\n",
      "19.6574575944\n",
      "19.6574567924\n",
      "19.6574591829\n",
      "19.657455204\n",
      "19.6574623532\n",
      "19.6574520338\n",
      "19.6574519085\n",
      "19.6574624787\n",
      "19.6574815418\n",
      "19.6574328457\n",
      "19.6574365659\n",
      "19.6574778209\n",
      "19.6574431251\n",
      "19.6574712617\n",
      "19.6574576967\n",
      "19.6574566901\n",
      "19.6574592887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6574550982\n",
      "19.6574624655\n",
      "19.6574519215\n",
      "19.6574519984\n",
      "19.6574623889\n",
      "19.6574816887\n",
      "19.6574326987\n",
      "19.6574366786\n",
      "19.6574777082\n",
      "19.6574431431\n",
      "19.6574712437\n",
      "19.6574577991\n",
      "19.6574565878\n",
      "19.6574593945\n",
      "19.6574549924\n",
      "19.6574625778\n",
      "19.6574518092\n",
      "19.6574520882\n",
      "19.657462299\n",
      "19.6574818357\n",
      "19.6574325517\n",
      "19.6574367913\n",
      "19.6574775955\n",
      "19.6574431612\n",
      "19.6574712256\n",
      "19.6574579014\n",
      "19.6574564855\n",
      "19.6574595003\n",
      "19.6574548866\n",
      "19.6574626901\n",
      "19.6574516969\n",
      "19.6574521781\n",
      "19.6574622092\n",
      "19.6574819826\n",
      "19.6574324048\n",
      "19.657436904\n",
      "19.6574774828\n",
      "19.6574431793\n",
      "19.6574712075\n",
      "19.6574580037\n",
      "19.6574563832\n",
      "19.6574596061\n",
      "19.6574547808\n",
      "19.6574628024\n",
      "19.6574515846\n",
      "19.6574522679\n",
      "19.6574621193\n",
      "19.6574821296\n",
      "19.6574322578\n",
      "19.6574370167\n",
      "19.6574773701\n",
      "19.6574431973\n",
      "19.6574711895\n",
      "19.657458106\n",
      "19.6574562809\n",
      "19.6574597119\n",
      "19.6574546751\n",
      "19.6574629147\n",
      "19.6574514723\n",
      "19.6574523578\n",
      "19.6574620295\n",
      "19.6574822766\n",
      "19.6574321109\n",
      "19.6574371293\n",
      "19.6574772574\n",
      "19.6574432154\n",
      "19.6574711714\n",
      "19.6574582083\n",
      "19.6574561786\n",
      "19.6574598177\n",
      "19.6574545693\n",
      "19.657463027\n",
      "19.65745136\n",
      "19.6574524476\n",
      "19.6574619396\n",
      "19.6574824235\n",
      "19.6574319639\n",
      "19.657437242\n",
      "19.6574771448\n",
      "19.6574432335\n",
      "19.6574711533\n",
      "19.6574583106\n",
      "19.6574560763\n",
      "19.6574599234\n",
      "19.6574544635\n",
      "19.6574631393\n",
      "19.6574512477\n",
      "19.6574525375\n",
      "19.6574618498\n",
      "19.6574825705\n",
      "19.6574318169\n",
      "19.6574373547\n",
      "19.6574770321\n",
      "19.6574432515\n",
      "19.6574711353\n",
      "19.6574584129\n",
      "19.657455974\n",
      "19.6574600292\n",
      "19.6574543577\n",
      "19.6574632516\n",
      "19.6574511354\n",
      "19.6574526274\n",
      "19.6574617599\n",
      "19.6574827175\n",
      "19.65743167\n",
      "19.6574374674\n",
      "19.6574769194\n",
      "19.6574432696\n",
      "19.6574711172\n",
      "19.6574585152\n",
      "19.6574558717\n",
      "19.657460135\n",
      "19.6574542519\n",
      "19.6574633639\n",
      "19.6574510231\n",
      "19.6574527172\n",
      "19.65746167\n",
      "19.6574828644\n",
      "19.657431523\n",
      "19.6574375801\n",
      "19.6574768067\n",
      "19.6574432877\n",
      "19.6574710991\n",
      "19.6574586175\n",
      "19.6574557693\n",
      "19.6574602408\n",
      "19.6574541461\n",
      "19.6574634762\n",
      "19.6574509108\n",
      "19.6574528071\n",
      "19.6574615802\n",
      "19.6574830114\n",
      "19.6574313761\n",
      "19.6574376928\n",
      "19.657476694\n",
      "19.6574433057\n",
      "19.6574710811\n",
      "19.6574587198\n",
      "19.657455667\n",
      "19.6574603466\n",
      "19.6574540403\n",
      "19.6574635885\n",
      "19.6574507986\n",
      "19.6574528969\n",
      "19.6574614903\n",
      "19.6574831583\n",
      "19.6574312291\n",
      "19.6574378054\n",
      "19.6574765813\n",
      "19.6574433238\n",
      "19.657471063\n",
      "19.6574588221\n",
      "19.6574555647\n",
      "19.6574604524\n",
      "19.6574539345\n",
      "19.6574637008\n",
      "19.6574506863\n",
      "19.6574529868\n",
      "19.6574614005\n",
      "19.6574833053\n",
      "19.6574310821\n",
      "19.6574379181\n",
      "19.6574764686\n",
      "19.6574433419\n",
      "19.6574710449\n",
      "19.6574589244\n",
      "19.6574554624\n",
      "19.6574605582\n",
      "19.6574538287\n",
      "19.6574638131\n",
      "19.657450574\n",
      "19.6574530766\n",
      "19.6574613106\n",
      "19.6574834523\n",
      "19.6574309352\n",
      "19.6574380308\n",
      "19.657476356\n",
      "19.6574433599\n",
      "19.6574710269\n",
      "19.6574590267\n",
      "19.6574553601\n",
      "19.657460664\n",
      "19.6574537229\n",
      "19.6574639254\n",
      "19.6574504617\n",
      "19.6574531665\n",
      "19.6574612208\n",
      "19.6574835992\n",
      "19.6574307882\n",
      "19.6574381435\n",
      "19.6574762433\n",
      "19.657443378\n",
      "19.6574710088\n",
      "19.657459129\n",
      "19.6574552578\n",
      "19.6574607698\n",
      "19.6574536171\n",
      "19.6574640377\n",
      "19.6574503494\n",
      "19.6574532564\n",
      "19.6574611309\n",
      "19.6574837462\n",
      "19.6574306413\n",
      "19.6574382562\n",
      "19.6574761306\n",
      "19.6574433961\n",
      "19.6574709907\n",
      "19.6574592313\n",
      "19.6574551555\n",
      "19.6574608756\n",
      "19.6574535113\n",
      "19.65746415\n",
      "19.6574502371\n",
      "19.6574533462\n",
      "19.6574610411\n",
      "19.6574838932\n",
      "19.6574304943\n",
      "19.6574383689\n",
      "19.6574760179\n",
      "19.6574434141\n",
      "19.6574709727\n",
      "19.6574593336\n",
      "19.6574550532\n",
      "19.6574609814\n",
      "19.6574534055\n",
      "19.6574642623\n",
      "19.6574501248\n",
      "19.6574534361\n",
      "19.6574609512\n",
      "19.6574840401\n",
      "19.6574303473\n",
      "19.6574384816\n",
      "19.6574759052\n",
      "19.6574434322\n",
      "19.6574709546\n",
      "19.6574594359\n",
      "19.6574549509\n",
      "19.6574610872\n",
      "19.6574532997\n",
      "19.6574643746\n",
      "19.6574500125\n",
      "19.6574535259\n",
      "19.6574608614\n",
      "19.6574841871\n",
      "19.6574302004\n",
      "19.6574385942\n",
      "19.6574757925\n",
      "19.6574434503\n",
      "19.6574709365\n",
      "19.6574595382\n",
      "19.6574548486\n",
      "19.657461193\n",
      "19.6574531939\n",
      "19.6574644869\n",
      "19.6574499002\n",
      "19.6574536158\n",
      "19.6574607715\n",
      "19.657484334\n",
      "19.6574300534\n",
      "19.6574387069\n",
      "19.6574756799\n",
      "19.6574434683\n",
      "19.6574709185\n",
      "19.6574596406\n",
      "19.6574547463\n",
      "19.6574612988\n",
      "19.6574530882\n",
      "19.6574645992\n",
      "19.6574497879\n",
      "19.6574537056\n",
      "19.6574606816\n",
      "19.657484481\n",
      "19.6574299064\n",
      "19.6574388196\n",
      "19.6574755672\n",
      "19.6574434864\n",
      "19.6574709004\n",
      "19.6574597429\n",
      "19.657454644\n",
      "19.6574614046\n",
      "19.6574529824\n",
      "19.6574647115\n",
      "19.6574496756\n",
      "19.6574537955\n",
      "19.6574605918\n",
      "19.657484628\n",
      "19.6574297595\n",
      "19.6574389323\n",
      "19.6574754545\n",
      "19.6574435045\n",
      "19.6574708823\n",
      "19.6574598452\n",
      "19.6574545417\n",
      "19.6574615104\n",
      "19.6574528766\n",
      "19.6574648238\n",
      "19.6574495633\n",
      "19.6574538854\n",
      "19.6574605019\n",
      "19.6574847749\n",
      "19.6574296125\n",
      "19.657439045\n",
      "19.6574753418\n",
      "19.6574435225\n",
      "19.6574708643\n",
      "19.6574599475\n",
      "19.6574544394\n",
      "19.6574616162\n",
      "19.6574527708\n",
      "19.6574649361\n",
      "19.657449451\n",
      "19.6574539752\n",
      "19.6574604121\n",
      "19.6574849219\n",
      "19.6574294656\n",
      "19.6574391577\n",
      "19.6574752291\n",
      "19.6574435406\n",
      "19.6574708462\n",
      "19.6574600498\n",
      "19.6574543371\n",
      "19.657461722\n",
      "19.657452665\n",
      "19.6574650484\n",
      "19.6574493387\n",
      "19.6574540651\n",
      "19.6574603222\n",
      "19.6574850689\n",
      "19.6574293186\n",
      "19.6574392704\n",
      "19.6574751164\n",
      "19.6574435587\n",
      "19.6574708281\n",
      "19.6574601521\n",
      "19.6574542348\n",
      "19.6574618277\n",
      "19.6574525592\n",
      "19.6574651607\n",
      "19.6574492264\n",
      "19.6574541549\n",
      "19.6574602324\n",
      "19.6574852158\n",
      "19.6574291716\n",
      "19.657439383\n",
      "19.6574750037\n",
      "19.6574435767\n",
      "19.6574708101\n",
      "19.6574602544\n",
      "19.6574541325\n",
      "19.6574619335\n",
      "19.6574524534\n",
      "19.657465273\n",
      "19.6574491141\n",
      "19.6574542448\n",
      "19.6574601425\n",
      "19.6574853628\n",
      "19.6574290247\n",
      "19.6574394957\n",
      "19.6574748911\n",
      "19.6574435948\n",
      "19.657470792\n",
      "19.6574603567\n",
      "19.6574540302\n",
      "19.6574620393\n",
      "19.6574523476\n",
      "19.6574653853\n",
      "19.6574490018\n",
      "19.6574543346\n",
      "19.6574600527\n",
      "19.6574855097\n",
      "19.6574288777\n",
      "19.6574396084\n",
      "19.6574747784\n",
      "19.6574436129\n",
      "19.6574707739\n",
      "19.657460459\n",
      "19.6574539278\n",
      "19.6574621451\n",
      "19.6574522418\n",
      "19.6574654976\n",
      "19.6574488895\n",
      "19.6574544245\n",
      "19.6574599628\n",
      "19.6574856567\n",
      "19.6574287308\n",
      "19.6574397211\n",
      "19.6574746657\n",
      "19.6574436309\n",
      "19.6574707559\n",
      "19.6574605613\n",
      "19.6574538255\n",
      "19.6574622509\n",
      "19.657452136\n",
      "19.6574656099\n",
      "19.6574487772\n",
      "19.6574545143\n",
      "19.657459873\n",
      "19.6574858037\n",
      "19.6574285838\n",
      "19.6574398338\n",
      "19.657474553\n",
      "19.657443649\n",
      "19.6574707378\n",
      "19.6574606636\n",
      "19.6574537232\n",
      "19.6574623567\n",
      "19.6574520302\n",
      "19.6574657222\n",
      "19.6574486649\n",
      "19.6574546042\n",
      "19.6574597831\n",
      "19.6574859506\n",
      "19.6574284368\n",
      "19.6574399465\n",
      "19.6574744403\n",
      "19.6574436671\n",
      "19.6574707197\n",
      "19.6574607659\n",
      "19.6574536209\n",
      "19.6574624625\n",
      "19.6574519244\n",
      "19.6574658345\n",
      "19.6574485526\n",
      "19.6574546941\n",
      "19.6574596932\n",
      "19.6574860976\n",
      "19.6574282899\n",
      "19.6574400591\n",
      "19.6574743276\n",
      "19.6574436851\n",
      "19.6574707017\n",
      "19.6574608682\n",
      "19.6574535186\n",
      "19.6574625683\n",
      "19.6574518186\n",
      "19.6574659468\n",
      "19.6574484403\n",
      "19.6574547839\n",
      "19.6574596034\n",
      "19.6574862446\n",
      "19.6574281429\n",
      "19.6574401718\n",
      "19.657474215\n",
      "19.6574437032\n",
      "19.6574706836\n",
      "19.6574609705\n",
      "19.6574534163\n",
      "19.6574626741\n",
      "19.6574517128\n",
      "19.6574660591\n",
      "19.657448328\n",
      "19.6574548738\n",
      "19.6574595135\n",
      "19.6574863915\n",
      "19.657427996\n",
      "19.6574402845\n",
      "19.6574741023\n",
      "19.6574437213\n",
      "19.6574706656\n",
      "19.6574610728\n",
      "19.657453314\n",
      "19.6574627799\n",
      "19.657451607\n",
      "19.6574661714\n",
      "19.6574482157\n",
      "19.6574549636\n",
      "19.6574594237\n",
      "19.6574865385\n",
      "19.657427849\n",
      "19.6574403972\n",
      "19.6574739896\n",
      "19.6574437393\n",
      "19.6574706475\n",
      "19.6574611751\n",
      "19.6574532117\n",
      "19.6574628857\n",
      "19.6574515012\n",
      "19.6574662837\n",
      "19.6574481034\n",
      "19.6574550535\n",
      "19.6574593338\n",
      "19.6574866854\n",
      "19.657427702\n",
      "19.6574405099\n",
      "19.6574738769\n",
      "19.6574437574\n",
      "19.6574706294\n",
      "19.6574612774\n",
      "19.6574531094\n",
      "19.6574629915\n",
      "19.6574513955\n",
      "19.657466396\n",
      "19.6574479911\n",
      "19.6574551433\n",
      "19.657459244\n",
      "19.6574868324\n",
      "19.6574275551\n",
      "19.6574406226\n",
      "19.6574737642\n",
      "19.6574437755\n",
      "19.6574706114\n",
      "19.6574613798\n",
      "19.6574530071\n",
      "19.6574630973\n",
      "19.6574512897\n",
      "19.6574665083\n",
      "19.6574478788\n",
      "19.6574552332\n",
      "19.6574591541\n",
      "19.6574869794\n",
      "19.6574274081\n",
      "19.6574407353\n",
      "19.6574736515\n",
      "19.6574437935\n",
      "19.6574705933\n",
      "19.6574614821\n",
      "19.6574529048\n",
      "19.6574632031\n",
      "19.6574511839\n",
      "19.6574666206\n",
      "19.6574477665\n",
      "19.6574553231\n",
      "19.6574590643\n",
      "19.6574871263\n",
      "19.6574272612\n",
      "19.6574408479\n",
      "19.6574735388\n",
      "19.6574438116\n",
      "19.6574705752\n",
      "19.6574615844\n",
      "19.6574528025\n",
      "19.6574633089\n",
      "19.6574510781\n",
      "19.6574667329\n",
      "19.6574476542\n",
      "19.6574554129\n",
      "19.6574589744\n",
      "19.6574872733\n",
      "19.6574271142\n",
      "19.6574409606\n",
      "19.6574734262\n",
      "19.6574438296\n",
      "19.6574705572\n",
      "19.6574616867\n",
      "19.6574527002\n",
      "19.6574634147\n",
      "19.6574509723\n",
      "19.6574668452\n",
      "19.6574475419\n",
      "19.6574555028\n",
      "19.6574588846\n",
      "19.6574874203\n",
      "19.6574269672\n",
      "W2 relative error: 2.85e-08\n",
      "19.6574571933\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "19.6574572544\n",
      "19.6574571323\n",
      "b1 relative error: 1.09e-09\n",
      "19.6574571933\n",
      "19.657453944\n",
      "19.6574604427\n",
      "19.6574540246\n",
      "19.6574603621\n",
      "19.657457517\n",
      "19.6574568697\n",
      "19.6574578321\n",
      "19.6574565546\n",
      "19.6574584583\n",
      "19.6574559284\n",
      "19.6574563733\n",
      "19.6574580134\n",
      "19.6574622042\n",
      "19.6574521825\n",
      "b2 relative error: 7.76e-10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-3\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print('Testing initialization ... ')\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print('Testing test-time forward pass ... ')\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print('Testing training loss (no regularization)')\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "  print('Running numeric gradient check with reg = ', reg)\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 14700) loss: 2.300807\n",
      "(Epoch 0 / 60) train acc: 0.142000; val_acc: 0.162000\n",
      "(Epoch 1 / 60) train acc: 0.436000; val_acc: 0.443000\n",
      "(Epoch 2 / 60) train acc: 0.491000; val_acc: 0.476000\n",
      "(Iteration 501 / 14700) loss: 1.394358\n",
      "(Epoch 3 / 60) train acc: 0.491000; val_acc: 0.483000\n",
      "(Epoch 4 / 60) train acc: 0.546000; val_acc: 0.495000\n",
      "(Iteration 1001 / 14700) loss: 1.409690\n",
      "(Epoch 5 / 60) train acc: 0.564000; val_acc: 0.497000\n",
      "(Epoch 6 / 60) train acc: 0.546000; val_acc: 0.500000\n",
      "(Iteration 1501 / 14700) loss: 1.375375\n",
      "(Epoch 7 / 60) train acc: 0.577000; val_acc: 0.510000\n",
      "(Epoch 8 / 60) train acc: 0.554000; val_acc: 0.524000\n",
      "(Iteration 2001 / 14700) loss: 1.271511\n",
      "(Epoch 9 / 60) train acc: 0.594000; val_acc: 0.508000\n",
      "(Epoch 10 / 60) train acc: 0.596000; val_acc: 0.521000\n",
      "(Iteration 2501 / 14700) loss: 1.173279\n",
      "(Epoch 11 / 60) train acc: 0.617000; val_acc: 0.518000\n",
      "(Epoch 12 / 60) train acc: 0.611000; val_acc: 0.529000\n",
      "(Iteration 3001 / 14700) loss: 1.053749\n",
      "(Epoch 13 / 60) train acc: 0.600000; val_acc: 0.517000\n",
      "(Epoch 14 / 60) train acc: 0.591000; val_acc: 0.520000\n",
      "(Iteration 3501 / 14700) loss: 0.931826\n",
      "(Epoch 15 / 60) train acc: 0.648000; val_acc: 0.520000\n",
      "(Epoch 16 / 60) train acc: 0.620000; val_acc: 0.542000\n",
      "(Iteration 4001 / 14700) loss: 1.129385\n",
      "(Epoch 17 / 60) train acc: 0.625000; val_acc: 0.534000\n",
      "(Epoch 18 / 60) train acc: 0.672000; val_acc: 0.513000\n",
      "(Iteration 4501 / 14700) loss: 1.001407\n",
      "(Epoch 19 / 60) train acc: 0.643000; val_acc: 0.516000\n",
      "(Epoch 20 / 60) train acc: 0.672000; val_acc: 0.520000\n",
      "(Iteration 5001 / 14700) loss: 0.930544\n",
      "(Epoch 21 / 60) train acc: 0.662000; val_acc: 0.510000\n",
      "(Epoch 22 / 60) train acc: 0.672000; val_acc: 0.511000\n",
      "(Iteration 5501 / 14700) loss: 0.958545\n",
      "(Epoch 23 / 60) train acc: 0.673000; val_acc: 0.514000\n",
      "(Epoch 24 / 60) train acc: 0.666000; val_acc: 0.531000\n",
      "(Iteration 6001 / 14700) loss: 0.934401\n",
      "(Epoch 25 / 60) train acc: 0.690000; val_acc: 0.529000\n",
      "(Epoch 26 / 60) train acc: 0.696000; val_acc: 0.518000\n",
      "(Iteration 6501 / 14700) loss: 0.932811\n",
      "(Epoch 27 / 60) train acc: 0.692000; val_acc: 0.525000\n",
      "(Epoch 28 / 60) train acc: 0.679000; val_acc: 0.508000\n",
      "(Iteration 7001 / 14700) loss: 0.916205\n",
      "(Epoch 29 / 60) train acc: 0.688000; val_acc: 0.535000\n",
      "(Epoch 30 / 60) train acc: 0.686000; val_acc: 0.519000\n",
      "(Iteration 7501 / 14700) loss: 0.786386\n",
      "(Epoch 31 / 60) train acc: 0.722000; val_acc: 0.521000\n",
      "(Epoch 32 / 60) train acc: 0.687000; val_acc: 0.522000\n",
      "(Iteration 8001 / 14700) loss: 0.960182\n",
      "(Epoch 33 / 60) train acc: 0.718000; val_acc: 0.523000\n",
      "(Epoch 34 / 60) train acc: 0.707000; val_acc: 0.521000\n",
      "(Iteration 8501 / 14700) loss: 0.947691\n",
      "(Epoch 35 / 60) train acc: 0.694000; val_acc: 0.516000\n",
      "(Epoch 36 / 60) train acc: 0.722000; val_acc: 0.521000\n",
      "(Iteration 9001 / 14700) loss: 1.008503\n",
      "(Epoch 37 / 60) train acc: 0.711000; val_acc: 0.517000\n",
      "(Epoch 38 / 60) train acc: 0.707000; val_acc: 0.521000\n",
      "(Iteration 9501 / 14700) loss: 0.782803\n",
      "(Epoch 39 / 60) train acc: 0.697000; val_acc: 0.514000\n",
      "(Epoch 40 / 60) train acc: 0.710000; val_acc: 0.517000\n",
      "(Iteration 10001 / 14700) loss: 0.880695\n",
      "(Epoch 41 / 60) train acc: 0.747000; val_acc: 0.527000\n",
      "(Epoch 42 / 60) train acc: 0.735000; val_acc: 0.495000\n",
      "(Iteration 10501 / 14700) loss: 0.791840\n",
      "(Epoch 43 / 60) train acc: 0.743000; val_acc: 0.536000\n",
      "(Epoch 44 / 60) train acc: 0.745000; val_acc: 0.526000\n",
      "(Iteration 11001 / 14700) loss: 0.784212\n",
      "(Epoch 45 / 60) train acc: 0.762000; val_acc: 0.509000\n",
      "(Epoch 46 / 60) train acc: 0.753000; val_acc: 0.508000\n",
      "(Iteration 11501 / 14700) loss: 0.683279\n",
      "(Epoch 47 / 60) train acc: 0.741000; val_acc: 0.532000\n",
      "(Epoch 48 / 60) train acc: 0.733000; val_acc: 0.527000\n",
      "(Iteration 12001 / 14700) loss: 0.834642\n",
      "(Epoch 49 / 60) train acc: 0.739000; val_acc: 0.516000\n",
      "(Epoch 50 / 60) train acc: 0.760000; val_acc: 0.523000\n",
      "(Epoch 51 / 60) train acc: 0.737000; val_acc: 0.517000\n",
      "(Iteration 12501 / 14700) loss: 0.740214\n",
      "(Epoch 52 / 60) train acc: 0.738000; val_acc: 0.516000\n",
      "(Epoch 53 / 60) train acc: 0.777000; val_acc: 0.513000\n",
      "(Iteration 13001 / 14700) loss: 0.729343\n",
      "(Epoch 54 / 60) train acc: 0.747000; val_acc: 0.523000\n",
      "(Epoch 55 / 60) train acc: 0.769000; val_acc: 0.519000\n",
      "(Iteration 13501 / 14700) loss: 0.966173\n",
      "(Epoch 56 / 60) train acc: 0.748000; val_acc: 0.524000\n",
      "(Epoch 57 / 60) train acc: 0.748000; val_acc: 0.523000\n",
      "(Iteration 14001 / 14700) loss: 0.849325\n",
      "(Epoch 58 / 60) train acc: 0.748000; val_acc: 0.521000\n",
      "(Epoch 59 / 60) train acc: 0.745000; val_acc: 0.512000\n",
      "(Iteration 14501 / 14700) loss: 0.831331\n",
      "(Epoch 60 / 60) train acc: 0.750000; val_acc: 0.511000\n"
     ]
    }
   ],
   "source": [
    "from cs231n.solver import Solver\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "solver = Solver(model, data, update_rule='sgd', optim_config={'learning_rate': 1e-3}, lr_decay=0.95, num_epochs=60, batch_size=200,\n",
    "                    print_every=500, verbose=True)\n",
    "solver.train()\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAALJCAYAAAD8s2GkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAApikAHAAAAACmKQAcAAAAAKer/A6cOiHVH+1AEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x125b5c3d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch normalization; we will add those features soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. You will need to tweak the learning rate and initialization scale, but you should be able to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-4\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_scale = 1e-5\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inline question: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net?\n",
    "\n",
    "# Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print('next_w error: ', rel_error(next_w, expected_next_w))\n",
    "print('velocity error: ', rel_error(expected_velocity, config['velocity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation; you should see errors less than 1e-7\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('cache error: ', rel_error(expected_cache, config['cache']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print('next_w error: ', rel_error(expected_next_w, next_w))\n",
    "print('v error: ', rel_error(expected_v, config['v']))\n",
    "print('m error: ', rel_error(expected_m, config['m']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print('running with ', update_rule)\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print()\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in list(solvers.items()):\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
    "\n",
    "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test you model\n",
    "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
